{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c3bf8b2",
   "metadata": {},
   "source": [
    "# Delivery 3: Full Walkthrough\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e0c560",
   "metadata": {},
   "source": [
    "Akshata Sandeep Patki  \n",
    "\n",
    "Mtk Nr:- 414540"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4f1350a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your base path is at: ads_covid-19'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "if os.path.split(os.getcwd())[-1]=='notebooks':\n",
    "    os.chdir(\"../\")\n",
    "\n",
    "'Your base path is at: '+os.path.split(os.getcwd())[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb9118ba",
   "metadata": {},
   "source": [
    "# Update the data from johns hopkins git repo and get current data from germany"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac506ef9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error : b\"Cloning into 'COVID-19'...\\nUpdating files:  11% (235/1978)\\rUpdating files:  12% (238/1978)\\rUpdating files:  13% (258/1978)\\rUpdating files:  14% (277/1978)\\rUpdating files:  15% (297/1978)\\rUpdating files:  16% (317/1978)\\rUpdating files:  17% (337/1978)\\rUpdating files:  18% (357/1978)\\rUpdating files:  19% (376/1978)\\rUpdating files:  19% (382/1978)\\rUpdating files:  20% (396/1978)\\rUpdating files:  21% (416/1978)\\rUpdating files:  22% (436/1978)\\rUpdating files:  23% (455/1978)\\rUpdating files:  24% (475/1978)\\rUpdating files:  25% (495/1978)\\rUpdating files:  26% (515/1978)\\rUpdating files:  27% (535/1978)\\rUpdating files:  28% (554/1978)\\rUpdating files:  28% (568/1978)\\rUpdating files:  29% (574/1978)\\rUpdating files:  30% (594/1978)\\rUpdating files:  31% (614/1978)\\rUpdating files:  32% (633/1978)\\rUpdating files:  33% (653/1978)\\rUpdating files:  34% (673/1978)\\rUpdating files:  35% (693/1978)\\rUpdating files:  36% (713/1978)\\rUpdating files:  36% (715/1978)\\rUpdating files:  37% (732/1978)\\rUpdating files:  38% (752/1978)\\rUpdating files:  39% (772/1978)\\rUpdating files:  40% (792/1978)\\rUpdating files:  41% (811/1978)\\rUpdating files:  42% (831/1978)\\rUpdating files:  42% (837/1978)\\rUpdating files:  43% (851/1978)\\rUpdating files:  44% (871/1978)\\rUpdating files:  45% (891/1978)\\rUpdating files:  46% (910/1978)\\rUpdating files:  47% (930/1978)\\rUpdating files:  47% (944/1978)\\rUpdating files:  48% (950/1978)\\rUpdating files:  49% (970/1978)\\rUpdating files:  50% (989/1978)\\rUpdating files:  51% (1009/1978)\\rUpdating files:  52% (1029/1978)\\rUpdating files:  53% (1049/1978)\\rUpdating files:  54% (1069/1978)\\rUpdating files:  55% (1088/1978)\\rUpdating files:  56% (1108/1978)\\rUpdating files:  57% (1128/1978)\\rUpdating files:  58% (1148/1978)\\rUpdating files:  59% (1168/1978)\\rUpdating files:  60% (1187/1978)\\rUpdating files:  61% (1207/1978)\\rUpdating files:  62% (1227/1978)\\rUpdating files:  63% (1247/1978)\\rUpdating files:  64% (1266/1978)\\rUpdating files:  65% (1286/1978)\\rUpdating files:  66% (1306/1978)\\rUpdating files:  67% (1326/1978)\\rUpdating files:  68% (1346/1978)\\rUpdating files:  69% (1365/1978)\\rUpdating files:  70% (1385/1978)\\rUpdating files:  71% (1405/1978)\\rUpdating files:  72% (1425/1978)\\rUpdating files:  73% (1444/1978)\\rUpdating files:  74% (1464/1978)\\rUpdating files:  75% (1484/1978)\\rUpdating files:  76% (1504/1978)\\rUpdating files:  77% (1524/1978)\\rUpdating files:  78% (1543/1978)\\rUpdating files:  79% (1563/1978)\\rUpdating files:  80% (1583/1978)\\rUpdating files:  81% (1603/1978)\\rUpdating files:  82% (1622/1978)\\rUpdating files:  83% (1642/1978)\\rUpdating files:  84% (1662/1978)\\rUpdating files:  85% (1682/1978)\\rUpdating files:  86% (1702/1978)\\rUpdating files:  87% (1721/1978)\\rUpdating files:  88% (1741/1978)\\rUpdating files:  89% (1761/1978)\\rUpdating files:  90% (1781/1978)\\rUpdating files:  91% (1800/1978)\\rUpdating files:  92% (1820/1978)\\rUpdating files:  92% (1822/1978)\\rUpdating files:  93% (1840/1978)\\rUpdating files:  94% (1860/1978)\\rUpdating files:  95% (1880/1978)\\rUpdating files:  96% (1899/1978)\\rUpdating files:  97% (1919/1978)\\rUpdating files:  98% (1939/1978)\\rUpdating files:  99% (1959/1978)\\rUpdating files: 100% (1978/1978)\\rUpdating files: 100% (1978/1978), done.\\n\"\n",
      "out : b''\n",
      " Number of regions rows: 411\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "def get_johns_hopkins():\n",
    "    ''' Get data by a git pull request, the source code has to be pulled first\n",
    "        Result is stored in the predifined csv structure\n",
    "    '''\n",
    "    git_repo = 'https://github.com/CSSEGISandData/COVID-19.git'\n",
    "    path = \"data/raw/COVID-19/\"\n",
    "    git_pull = subprocess.Popen( \"git clone \"+ git_repo,\n",
    "                         cwd = os.path.dirname( os.path.realpath(path)),\n",
    "                         shell = True,\n",
    "                         stdout = subprocess.PIPE,\n",
    "                         stderr = subprocess.PIPE )\n",
    "    (out, error) = git_pull.communicate()\n",
    "\n",
    "\n",
    "    print(\"Error : \" + str(error))\n",
    "    print(\"out : \" + str(out))\n",
    "\n",
    "\n",
    "def get_current_data_germany():\n",
    "    ''' Get current data from germany, attention API endpoint not too stable\n",
    "        Result data frame is stored as pd.DataFrame\n",
    "\n",
    "    '''\n",
    "    # 16 states\n",
    "    #data=requests.get('https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/Coronaf%C3%A4lle_in_den_Bundesl%C3%A4ndern/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json')\n",
    "\n",
    "    # 400 regions / Landkreise\n",
    "    data=requests.get('https://services7.arcgis.com/mOBPykOjAyBO2ZKk/arcgis/rest/services/RKI_Landkreisdaten/FeatureServer/0/query?where=1%3D1&outFields=*&outSR=4326&f=json')\n",
    "\n",
    "    json_object=json.loads(data.content)\n",
    "    full_list=[]\n",
    "    for pos,each_dict in enumerate (json_object['features'][:]):\n",
    "        full_list.append(each_dict['attributes'])\n",
    "\n",
    "    pd_full_list=pd.DataFrame(full_list)\n",
    "    pd_full_list.to_csv('data/raw/NPGEO/GER_state_data.csv')\n",
    "    print(' Number of regions rows: '+str(pd_full_list.shape[0]))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    get_johns_hopkins()\n",
    "    get_current_data_germany()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98941ebe",
   "metadata": {},
   "source": [
    "# Required Data set preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "566bbc3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of rows stored: 3317790\n",
      " Latest date is: 2022-07-24 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "def store_relational_JH_data():\n",
    "    ''' Transformes the COVID data in a relational data set\n",
    "\n",
    "    '''\n",
    "\n",
    "data_path='data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv'\n",
    "pd_raw=pd.read_csv(data_path)\n",
    "pd_data_base=pd_raw.rename(columns={'Country/Region':'Country',\n",
    "                      'Province/State':'State'})\n",
    "\n",
    "pd_data_base['State']=pd_data_base['State'].fillna('no')\n",
    "\n",
    "pd_data_base=pd_data_base.drop(['Lat','Long'],axis=1)\n",
    "pd_relational_model=pd_data_base.set_index(['State','Country']) \\\n",
    "                                .T                              \\\n",
    "                                .stack(level=[0,1])             \\\n",
    "                                .reset_index()                  \\\n",
    "                                .rename(columns={'level_0':'date',\n",
    "                                                   0:'Confirmed'},\n",
    "                                                  )\n",
    "pd_relational_model['date']=pd_relational_model.date.astype('datetime64[ns]')\n",
    "pd_relational_model.to_csv('../ads_covid-19/data/processed/COVID_relational_confirmed.csv',index=False)\n",
    "\n",
    "\n",
    "## Analyzing data for Country US as it is in a new file due to the new structure of John Hopkins\n",
    "\n",
    "data_path_US='data/raw/COVID-19/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_US.csv'\n",
    "pd_raw_US=pd.read_csv(data_path_US)\n",
    "\n",
    "pd_raw_US=pd_raw_US.drop(['UID', 'iso2', 'iso3', 'code3', 'Country_Region','FIPS', 'Admin2',  'Lat', 'Long_', 'Combined_Key'],axis=1)\n",
    "pd_data_base_US=pd_raw_US.rename(columns={'Province_State':'State'}).copy()\n",
    "pd_relation_model_US=pd_data_base_US.set_index(['State']).T\n",
    "pd_relation_model_US=pd_relation_model_US.stack().reset_index().rename(columns={'level_0':'date',0:'Confirmed'})\n",
    "pd_relation_model_US['Country']='US'\n",
    "pd_relation_model_US['date']=[datetime.strptime( each,\"%m/%d/%y\") for each in pd_relation_model_US.date]\n",
    "\n",
    "##Swap the columns according to the prvious stored data in pd_relational_model\n",
    "\n",
    "columns_titles = [\"date\",\"State\",\"Country\",\"Confirmed\"]\n",
    "pd_relation_model_US=pd_relation_model_US.reindex(columns=columns_titles)\n",
    "\n",
    "## Merge the US data\n",
    "pd_relational_model_all=pd_relational_model[pd_relational_model['Country']!='US'].reset_index(drop=True)\n",
    "pd_relational_model_all=pd.concat([pd_relational_model_all,pd_relation_model_US],ignore_index=True)\n",
    "\n",
    "## Save the complete data\n",
    "pd_relational_model_all.to_csv('data/processed/20220719_COVID_relational_confirmed.csv',index=False)\n",
    "\n",
    "print(' Number of rows stored: '+str(pd_relational_model_all.shape[0]))\n",
    "print(' Latest date is: '+str(max(pd_relational_model_all.date)))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    store_relational_JH_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc1c0e6",
   "metadata": {},
   "source": [
    "# Slope Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a2b2452",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the test slope is: [2.]\n",
      "              date State  Country   Confirmed  Confirmed_filtered  \\\n",
      "3198835 2022-07-20    no  Germany  30131303.0          30109983.0   \n",
      "3198836 2022-07-21    no  Germany  30239122.0          30205473.6   \n",
      "3198837 2022-07-22    no  Germany  30331131.0          30272764.4   \n",
      "3198838 2022-07-23    no  Germany  30331133.0          30321931.5   \n",
      "3198839 2022-07-24    no  Germany  30331133.0          30371098.6   \n",
      "\n",
      "         Confirmed_DR  Confirmed_filtered_DR  \n",
      "3198835  2.160716e+02             253.222339  \n",
      "3198836  2.464517e+02             269.804637  \n",
      "3198837  3.025988e+02             371.001523  \n",
      "3198838  6.586269e+02             519.788235  \n",
      "3198839  3.033113e+07             616.711815  \n"
     ]
    }
   ],
   "source": [
    "# %load src/features/build_features.py\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression(fit_intercept=True)\n",
    "import pandas as pd\n",
    "\n",
    "from scipy import signal\n",
    "\n",
    "\n",
    "def get_doubling_time_via_regression(in_array):\n",
    "    ''' Use a linear regression to approximate the doubling rate\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        in_array : pandas.series\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        Doubling rate: double\n",
    "    '''\n",
    "\n",
    "    y = np.array(in_array)\n",
    "    X = np.arange(-1,2).reshape(-1, 1)\n",
    "\n",
    "    assert len(in_array)==3\n",
    "    reg.fit(X,y)\n",
    "    intercept=reg.intercept_\n",
    "    slope=reg.coef_\n",
    "\n",
    "    return intercept/slope\n",
    "\n",
    "\n",
    "def savgol_filter(df_input,column='Confirmed',window=5):\n",
    "    ''' Savgol Filter which can be used in groupby apply function (data structure kept)\n",
    "\n",
    "        parameters:\n",
    "        ----------\n",
    "        df_input : pandas.series\n",
    "        column : str\n",
    "        window : int\n",
    "            used data points to calculate the filter result\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_result: pd.DataFrame\n",
    "            the index of the df_input has to be preserved in result\n",
    "    '''\n",
    "\n",
    "    degree=1\n",
    "    df_result=df_input\n",
    "\n",
    "    filter_in=df_input[column].fillna(0) # attention with the neutral element here\n",
    "\n",
    "    result=signal.savgol_filter(np.array(filter_in),\n",
    "                           window, # window size used for filtering\n",
    "                           1)\n",
    "    df_result[str(column+'_filtered')]=result\n",
    "    return df_result\n",
    "\n",
    "def rolling_reg(df_input,col='Confirmed'):\n",
    "    ''' Rolling Regression to approximate the doubling time'\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        col: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        result: pd.DataFrame\n",
    "    '''\n",
    "    days_back=3\n",
    "    result=df_input[col].rolling(\n",
    "                window=days_back,\n",
    "                min_periods=days_back).apply(get_doubling_time_via_regression,raw=False)\n",
    "\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_filtered_data(df_input,filter_on='Confirmed'):\n",
    "    '''  Calculate savgol filter and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['State','Country',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "    df_output=df_input.copy() # we need a copy here otherwise the filter_on column will be overwritten\n",
    "\n",
    "    pd_filtered_result=df_output[['State','Country',filter_on]].groupby(['State','Country']).apply(savgol_filter)#.reset_index()\n",
    "\n",
    "    #print('--+++ after group by apply')\n",
    "    #print(pd_filtered_result[pd_filtered_result['country']=='Germany'].tail())\n",
    "\n",
    "    #df_output=pd.merge(df_output,pd_filtered_result[['index',str(filter_on+'_filtered')]],on=['index'],how='left')\n",
    "    df_output=pd.merge(df_output,pd_filtered_result[[str(filter_on+'_filtered')]],left_index=True,right_index=True,how='left')\n",
    "    #print(df_output[df_output['country']=='Germany'].tail())\n",
    "    return df_output.copy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calc_doubling_rate(df_input,filter_on='Confirmed'):\n",
    "    ''' Calculate approximated doubling rate and return merged data frame\n",
    "\n",
    "        Parameters:\n",
    "        ----------\n",
    "        df_input: pd.DataFrame\n",
    "        filter_on: str\n",
    "            defines the used column\n",
    "        Returns:\n",
    "        ----------\n",
    "        df_output: pd.DataFrame\n",
    "            the result will be joined as a new column on the input data frame\n",
    "    '''\n",
    "\n",
    "    must_contain=set(['State','Country',filter_on])\n",
    "    assert must_contain.issubset(set(df_input.columns)), ' Erro in calc_filtered_data not all columns in data frame'\n",
    "\n",
    "\n",
    "    pd_DR_result= df_input.groupby(['State','Country']).apply(rolling_reg,filter_on).reset_index()\n",
    "\n",
    "    pd_DR_result=pd_DR_result.rename(columns={filter_on:filter_on+'_DR',\n",
    "                             'level_2':'index'})\n",
    "\n",
    "    #we do the merge on the index of our big table and on the index column after groupby\n",
    "    df_output=pd.merge(df_input,pd_DR_result[['index',str(filter_on+'_DR')]],left_index=True,right_on=['index'],how='left')\n",
    "    df_output=df_output.drop(columns=['index'])\n",
    "\n",
    "\n",
    "    return df_output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_data_reg=np.array([2,4,6])\n",
    "    result=get_doubling_time_via_regression(test_data_reg)\n",
    "    print('the test slope is: '+str(result))\n",
    "\n",
    "    pd_JH_data=pd.read_csv('data/processed/20220719_COVID_relational_confirmed.csv',parse_dates=[0])\n",
    "    pd_JH_data=pd_JH_data.sort_values('date',ascending=True).copy()\n",
    "\n",
    "    #test_structure=pd_JH_data[((pd_JH_data['country']=='US')|\n",
    "    #                  (pd_JH_data['country']=='Germany'))]\n",
    "\n",
    "    pd_result_larg=calc_filtered_data(pd_JH_data)\n",
    "    pd_result_larg=calc_doubling_rate(pd_result_larg)\n",
    "    pd_result_larg=calc_doubling_rate(pd_result_larg,'Confirmed_filtered')\n",
    "\n",
    "\n",
    "    mask=pd_result_larg['Confirmed']>100\n",
    "    pd_result_larg['Confirmed_filtered_DR']=pd_result_larg['Confirmed_filtered_DR'].where(mask, other=np.NaN)\n",
    "    pd_result_larg.to_csv('../ads_covid-19/data/processed/COVID_final_set.csv',index=False)\n",
    "    print(pd_result_larg[pd_result_larg['Country']=='Germany'].tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3cfaf6",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "419484fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\ads_covid-19\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m * Tip: There are .env or .flaskenv files present. Do \"pip install python-dotenv\" to use them.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    }
   ],
   "source": [
    "# %load src/visualization/visualize.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import dash\n",
    "dash.__version__\n",
    "#import dash_core_components as dcc\n",
    "from dash import dcc\n",
    "#import dash_html_components as html\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output,State\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import os\n",
    "print(os.getcwd())\n",
    "\n",
    "df_input_large=pd.read_csv('data/processed/COVID_final_set.csv')\n",
    "\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "app = dash.Dash()\n",
    "app.layout = html.Div([\n",
    "\n",
    "    dcc.Markdown('''\n",
    "    #  Statistical Analysis of COVID-19 data using Data Science\n",
    "\n",
    "    Goal of the project is to teach data science by applying a cross industry standard process,\n",
    "    it covers the full walkthrough of: automated data gathering, data transformations,\n",
    "    filtering and machine learning to approximating the doubling time, and\n",
    "    (static) deployment of responsive dashboard.\n",
    "\n",
    "    '''),\n",
    "\n",
    "    dcc.Markdown('''\n",
    "    ## Multi-Select Country for visualization\n",
    "    '''),\n",
    "\n",
    "\n",
    "    dcc.Dropdown(\n",
    "        id='country_drop_down',\n",
    "        options=[ {'label': each,'value':each} for each in df_input_large['Country'].unique()],\n",
    "        value=['India', 'Germany','Italy'], # which are pre-selected\n",
    "        multi=True\n",
    "    ),\n",
    "\n",
    "    dcc.Markdown('''\n",
    "        ## Select Timeline of confirmed COVID-19 cases or the approximated doubling time\n",
    "        '''),\n",
    "\n",
    "\n",
    "    dcc.Dropdown(\n",
    "    id='doubling_time',\n",
    "    options=[\n",
    "        {'label': 'Timeline Confirmed ', 'value': 'Confirmed'},\n",
    "        {'label': 'Timeline Confirmed Filtered', 'value': 'Confirmed_filtered'},\n",
    "        {'label': 'Timeline Doubling Rate', 'value': 'Confirmed_DR'},\n",
    "        {'label': 'Timeline Doubling Rate Filtered', 'value': 'Confirmed_filtered_DR'},\n",
    "    ],\n",
    "    value='Confirmed',\n",
    "    multi=False\n",
    "    ),\n",
    "\n",
    "    dcc.Graph(figure=fig, id='main_window_slope')\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "@app.callback(\n",
    "    Output('main_window_slope', 'figure'),\n",
    "    [Input('country_drop_down', 'value'),\n",
    "    Input('doubling_time', 'value')])\n",
    "def update_figure(country_list,show_doubling):\n",
    "\n",
    "\n",
    "    if 'DR' in show_doubling:\n",
    "        my_yaxis={'type':\"log\",\n",
    "               'title':'Approximated doubling rate over 3 days (#stayathome)'\n",
    "              }\n",
    "    else:\n",
    "        my_yaxis={'type':\"log\",\n",
    "                  'title':'Confirmed infected people (source johns hopkins csse, log-scale)'\n",
    "              }\n",
    "\n",
    "\n",
    "    traces = []\n",
    "    for each in country_list:\n",
    "\n",
    "        df_plot=df_input_large[df_input_large['Country']==each]\n",
    "\n",
    "        if show_doubling=='Confirmed_filtered_DR':\n",
    "            df_plot=df_plot[['State','Country','Confirmed','Confirmed_filtered','Confirmed_DR','Confirmed_filtered_DR','date']].groupby(['Country','date']).agg(np.mean).reset_index()\n",
    "        else:\n",
    "            df_plot=df_plot[['State','Country','Confirmed','Confirmed_filtered','Confirmed_DR','Confirmed_filtered_DR','date']].groupby(['Country','date']).agg(np.sum).reset_index()\n",
    "       #print(show_doubling)\n",
    "\n",
    "\n",
    "        traces.append(dict(x=df_plot.date,\n",
    "                                y=df_plot[show_doubling],\n",
    "                                mode='markers+lines',\n",
    "                                line_width=1,\n",
    "                                opacity=0.9,\n",
    "                                name=each\n",
    "                        )\n",
    "                )\n",
    "\n",
    "    return {\n",
    "            'data': traces,\n",
    "            'layout': dict (\n",
    "                width=1280,\n",
    "                height=720,\n",
    "\n",
    "                xaxis={'title':'Timeline',\n",
    "                        'tickangle':-45,\n",
    "                        'nticks':20,\n",
    "                        'tickfont':dict(size=14,color=\"#7f7f7f\"),\n",
    "                      },\n",
    "\n",
    "                yaxis=my_yaxis\n",
    "        )\n",
    "    }\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    app.run_server(debug=True, use_reloader=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c50112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
